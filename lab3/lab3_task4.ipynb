{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Nim  \n",
    "## Task3.4: An agent using reinforcement learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from nim import Nimply, Nim\n",
    "from play_nim import opponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(message)s\", level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_id(state: list, player: int):\n",
    "  assert player == 1 or player == 0\n",
    "  return hash(tuple(sorted(state)) + (player, ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node class from Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "  \n",
    "  def __init__(self, state: list, player: int):\n",
    "    assert player == 1 or player == 0\n",
    "    \n",
    "    self.id = hash_id(state, player)\n",
    "    self.state = deepcopy(state)\n",
    "    self.player = player # Me (0) -> max ; Opponent (1) -> min\n",
    "    \n",
    "    self.reward = 0 \n",
    "    self.children = []\n",
    "    self.parents = []\n",
    "    self.possible_acitions() # creates self.actions\n",
    "\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    return isinstance(other, Node) and self.state == other.state and self.player == other.player\n",
    "\n",
    "\n",
    "  def link_parent(self, parent):\n",
    "    assert isinstance(parent, Node)\n",
    "    assert self.player != parent.player\n",
    "    if parent not in self.parents:\n",
    "      self.parents.append(parent)\n",
    "\n",
    "\n",
    "  def link_child(self, child):\n",
    "    assert isinstance(child, Node)\n",
    "    assert self.player != child.player\n",
    "    if child not in self.children:\n",
    "      self.children.append(child)\n",
    "\n",
    "\n",
    "  def is_game_over(self):\n",
    "    return sum(self.state) == 0\n",
    "\n",
    "  \n",
    "  def give_reward(self):\n",
    "    \"\"\"\n",
    "    - not end -> reward = -1  \n",
    "    - win -> reward = 100  \n",
    "    - lose -> reward = -100  \n",
    "    \"\"\"\n",
    "    if not self.is_game_over():\n",
    "      return -1\n",
    "    if self.player == 0: # I lose\n",
    "      return -100\n",
    "    return 100 # I win\n",
    "\n",
    "\n",
    "  def possible_acitions(self, k=None):\n",
    "    self.actions = []\n",
    "    \n",
    "    if self.is_game_over():\n",
    "      return\n",
    "\n",
    "    not_zero_rows = [(r, n) for r, n in enumerate(self.state) if n > 0]\n",
    "    for row, num_obj in not_zero_rows:  \n",
    "      while num_obj > 0:\n",
    "        if k and num_obj > k:\n",
    "          num_obj = k\n",
    "          continue\n",
    "        self.actions.append(Nimply(row, num_obj))\n",
    "        num_obj -= 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game Tree (builded recursively, such us in task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameTree():\n",
    "  \n",
    "  def __init__(self, nim: Nim, start_player=0):\n",
    "    self.k = nim._k\n",
    "    self.start_player = start_player\n",
    "    self.dict_id_node = {}    \n",
    "    self.dict_id_reward = {} \n",
    "    \n",
    "    self.root = Node(nim._rows, start_player)\n",
    "    self.dict_id_node[self.root.id] = self.root\n",
    "    logging.info(f'Building the tree...')\n",
    "    self.build_tree()\n",
    "    logging.info('Done')\n",
    "\n",
    "  \n",
    "  def build_tree(self):\n",
    "    def recursive(node: Node):\n",
    "      # Stop condition\n",
    "      if node.id in self.dict_id_reward:\n",
    "        return\n",
    "      \n",
    "      if node.is_game_over():\n",
    "        node.reward = node.give_reward()\n",
    "        self.dict_id_reward[node.id] = node.reward\n",
    "        return\n",
    "\n",
    "\n",
    "      # Recursive part\n",
    "      for ply in node.actions:\n",
    "        row, num_obj = ply\n",
    "        \n",
    "        # Check rules\n",
    "        assert node.state[row] >= num_obj\n",
    "        assert self.k is None or num_obj <= self.k\n",
    "\n",
    "        # Create the child\n",
    "        child_state = deepcopy(node.state)\n",
    "        child_state[row] -= num_obj # nimming\n",
    "        child_id = hash_id(child_state, 1 - node.player)\n",
    "        if child_id in self.dict_id_node: # node already exists\n",
    "          child = self.dict_id_node[child_id]\n",
    "        else: # create the new node\n",
    "          child = Node(child_state, 1 - node.player)\n",
    "          self.dict_id_node[child_id] = child\n",
    "        \n",
    "        # Link parent and child\n",
    "        node.link_child(child)\n",
    "        child.link_parent(node)\n",
    "\n",
    "        # Recursion\n",
    "        recursive(child)\n",
    "          \n",
    "      # Reward of the node (-1)\n",
    "      node.reward = node.give_reward()\n",
    "      self.dict_id_reward[node.id] = node.reward\n",
    "    \n",
    "      return \n",
    "\n",
    "    recursive(self.root)\n",
    "    self.root.reward = self.root.give_reward()\n",
    "  \n",
    "\n",
    "  def player_states(self, player):\n",
    "    assert player == 0 or player == 1\n",
    "\n",
    "    dict_player = {}\n",
    "    for (id, node) in self.dict_id_node.items():\n",
    "      if node.player == player:\n",
    "        dict_player[id] = node\n",
    "    \n",
    "    return dict_player\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "  \n",
    "  def __init__(self, game_tree: GameTree, alpha=0.5, random_factor=0.2):\n",
    "    self.alpha = alpha\n",
    "    self.random_factor = random_factor\n",
    "    #self.dict_id_node = game_tree.dict_id_node\n",
    "    \n",
    "    self.state_history = [game_tree.root] # node -> inside has state and reward\n",
    "    self.G = {} # (k, v) = id_node, expected reward\n",
    "    for id, node in game_tree.dict_id_node.items():\n",
    "        self.G[id] = random.uniform(1.0, 0.1)\n",
    "\n",
    "\n",
    "  def choose_action(self, node: Node):\n",
    "    maxG = -10e15\n",
    "    next_move = None\n",
    "    \n",
    "    # Random action\n",
    "    if random.random() < self.random_factor:\n",
    "      next_move = random.choice(node.actions)\n",
    "    # Action with highest G (reward)\n",
    "    else: \n",
    "      for a in node.actions:  # a is a Nimply obj\n",
    "        new_state = deepcopy(node.state)\n",
    "        new_state[a.row] -= a.num_objects\n",
    "        new_state_id = hash_id(new_state, player=1) # opponent's state\n",
    "        if self.G[new_state_id] >= maxG:\n",
    "          next_move = a\n",
    "          maxG = self.G[new_state_id]\n",
    "\n",
    "    return next_move      \n",
    "        \n",
    "\n",
    "  def update_history(self, node: Node):\n",
    "    self.state_history.append(node)\n",
    "\n",
    "\n",
    "  def learn(self):\n",
    "    target = 0\n",
    "\n",
    "    for node in reversed(self.state_history):\n",
    "      self.G[node.id] = self.G[node.id] + self.alpha * (target - self.G[node.id])\n",
    "      target += node.reward\n",
    "\n",
    "    self.state_history = []     # Restart\n",
    "    self.random_factor -= 10e-5 # Decrease random factor each episode of play\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_nim(nim: Nim, game_tree: GameTree, agent: Agent, opponent: callable, episodes = 5000 ):\n",
    "\n",
    "  for e in range(episodes):\n",
    "    # Play a game\n",
    "    episode_nim = deepcopy(nim)\n",
    "    state = game_tree.root\n",
    "    while not episode_nim.is_game_over():\n",
    "      # My turn\n",
    "      if state.player == 0:\n",
    "        my_action = agent.choose_action(state) # Choose an action\n",
    "        print(f'0: {my_action} on {episode_nim}')\n",
    "        episode_nim.nimming(my_action)         # Update the state\n",
    "        \n",
    "        # Get new state and reward\n",
    "        new_state_id = hash_id(episode_nim._rows, player = 1)\n",
    "        state = game_tree.dict_id_node[new_state_id]\n",
    "        agent.update_history(state)\n",
    "      \n",
    "      # Opponent turn\n",
    "      else:\n",
    "        opp_action = opponent(episode_nim)\n",
    "        print(f'1: {opp_action} on {episode_nim}')\n",
    "        episode_nim.nimming(opp_action)\n",
    "        new_state_id = hash_id(episode_nim._rows, player = 0)\n",
    "        state = game_tree.dict_id_node[new_state_id]\n",
    "      \n",
    "      # Log\n",
    "      if e % 50 == 0:\n",
    "        logging.info(f'Episode {e}: player {(1 - state.player)} on {state.state}')\n",
    "      \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building the tree...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "nim = Nim(5)\n",
    "game_tree = GameTree(nim, start_player=0)\n",
    "agent = Agent(game_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 0: player 0 on [1, 3, 4, 7, 9]\n",
      "Episode 0: player 1 on [0, 3, 4, 7, 9]\n",
      "Episode 0: player 0 on [0, 3, 3, 7, 9]\n",
      "Episode 0: player 1 on [0, 0, 3, 7, 9]\n",
      "Episode 0: player 0 on [0, 0, 1, 3, 9]\n",
      "Episode 0: player 1 on [0, 0, 1, 3, 6]\n",
      "Episode 0: player 0 on [0, 0, 1, 3, 3]\n",
      "Episode 0: player 1 on [0, 0, 1, 2, 3]\n",
      "Episode 0: player 0 on [0, 0, 1, 1, 2]\n",
      "Episode 0: player 1 on [0, 0, 1, 1, 1]\n",
      "Episode 0: player 0 on [0, 0, 0, 1, 1]\n",
      "Episode 0: player 1 on [0, 0, 0, 0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Nimply(row=2, num_objects=1) on <1 3 5 7 9>\n",
      "1: Nimply(row=0, num_objects=1) on <1 3 4 7 9>\n",
      "0: Nimply(row=2, num_objects=1) on <0 3 4 7 9>\n",
      "1: Nimply(row=1, num_objects=3) on <0 3 3 7 9>\n",
      "0: Nimply(row=3, num_objects=6) on <0 0 3 7 9>\n",
      "1: Nimply(row=4, num_objects=3) on <0 0 3 1 9>\n",
      "0: Nimply(row=4, num_objects=3) on <0 0 3 1 6>\n",
      "1: Nimply(row=2, num_objects=1) on <0 0 3 1 3>\n",
      "0: Nimply(row=4, num_objects=2) on <0 0 2 1 3>\n",
      "1: Nimply(row=2, num_objects=1) on <0 0 2 1 1>\n",
      "0: Nimply(row=4, num_objects=1) on <0 0 1 1 1>\n",
      "1: Nimply(row=2, num_objects=1) on <0 0 1 1 0>\n",
      "0: Nimply(row=4, num_objects=1) on <0 0 0 1 0>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luigi\\OneDrive\\Desktop\\Didattica\\Magistrale PoliTO\\Anno II\\Computational Intelligance\\Computational-Intelligence\\lab3\\lab3_task4.ipynb Cella 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luigi/OneDrive/Desktop/Didattica/Magistrale%20PoliTO/Anno%20II/Computational%20Intelligance/Computational-Intelligence/lab3/lab3_task4.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m opponent \u001b[39m=\u001b[39m opponents[\u001b[39m3\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luigi/OneDrive/Desktop/Didattica/Magistrale%20PoliTO/Anno%20II/Computational%20Intelligance/Computational-Intelligence/lab3/lab3_task4.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m RL_nim(nim, game_tree, agent, opponent\u001b[39m=\u001b[39;49mopponent )\n",
      "\u001b[1;32mc:\\Users\\luigi\\OneDrive\\Desktop\\Didattica\\Magistrale PoliTO\\Anno II\\Computational Intelligance\\Computational-Intelligence\\lab3\\lab3_task4.ipynb Cella 16\u001b[0m in \u001b[0;36mRL_nim\u001b[1;34m(nim, game_tree, agent, opponent, episodes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luigi/OneDrive/Desktop/Didattica/Magistrale%20PoliTO/Anno%20II/Computational%20Intelligance/Computational-Intelligence/lab3/lab3_task4.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m my_action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mchoose_action(state) \u001b[39m# Choose an action\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luigi/OneDrive/Desktop/Didattica/Magistrale%20PoliTO/Anno%20II/Computational%20Intelligance/Computational-Intelligence/lab3/lab3_task4.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0: \u001b[39m\u001b[39m{\u001b[39;00mmy_action\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mepisode_nim\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/luigi/OneDrive/Desktop/Didattica/Magistrale%20PoliTO/Anno%20II/Computational%20Intelligance/Computational-Intelligence/lab3/lab3_task4.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m episode_nim\u001b[39m.\u001b[39;49mnimming(my_action)         \u001b[39m# Update the state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luigi/OneDrive/Desktop/Didattica/Magistrale%20PoliTO/Anno%20II/Computational%20Intelligance/Computational-Intelligence/lab3/lab3_task4.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Get new state and reward\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luigi/OneDrive/Desktop/Didattica/Magistrale%20PoliTO/Anno%20II/Computational%20Intelligance/Computational-Intelligence/lab3/lab3_task4.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m new_state_id \u001b[39m=\u001b[39m hash_id(episode_nim\u001b[39m.\u001b[39m_rows, player \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luigi\\OneDrive\\Desktop\\Didattica\\Magistrale PoliTO\\Anno II\\Computational Intelligance\\Computational-Intelligence\\lab3\\nim.py:27\u001b[0m, in \u001b[0;36mNim.nimming\u001b[1;34m(self, ply)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnimming\u001b[39m(\u001b[39mself\u001b[39m, ply: Nimply) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     row, num_objects \u001b[39m=\u001b[39m ply\n\u001b[1;32m---> 27\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rows[row] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m num_objects\n\u001b[0;32m     28\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_k \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_objects \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_k\n\u001b[0;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rows[row] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m num_objects\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opponent = opponents[3]\n",
    "RL_nim(nim, game_tree, agent, opponent=opponent )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfdee70b8fde6354ca2c3454fb2ed6ad5a414021dbe15bd7e75ed280b1128364"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
